---
layout : single
title : "우당탕탕 Airflow 도입 과정"
categories: data
tags: [ data_pipeline, data_engineer, docker]
toc: true
author_profile: false
sidebar:
    nav: "counts"
---

오늘은 데이터 파이프라인 툴 Airflow 도입 과정을 설명하겠다. 도입한지는 한참됐지만 이제야 정리를 해본다 ... 

### Airflow란?
Airflow는 초기 에어비엔비(Airfbnb) 엔지니어링 팀에서 개발한 워크플로우 오픈 소스 플랫폼이다.<br/>
간단히 얘기해서 스케줄을 실행해주고 여러 스케줄들이 잘 돌아가는지 쉽게 모니터링 할 수 있는 웹서버 등도 제공해주는 플랫폼이라고 볼 수 있다. 보통 데이터 워크플로우를 개선하거나 데이터엔지니어링의 ETL 작업을 자동화하기 위해 많이 사용한다. 
<br/>
<br/>

### 도입 과정
Airflow를 운영하는 방법에는 여러가지가 있지만 나는 공식 홈페이지에도 나와있는 가장 간단한 방법으로 우선 진행했다. 이는 docker-compose를 활용하여 웹서버를 띄우는 방식인데, docker가 처음이어서 생소하긴 했지만 공식 문서로도 잘 설명되어 있어서 차근차근 따라해보았다. 

## Airflow 설치 
airflow를 docker-compose로 실행하기 위해 설치하는건 간단하다. 아래 내용을 따라해보자 

```
//airflow docker-compose 생성
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.9.3/docker-compose.yaml'

//디렉토리 생성 및 airflow UID 확인
mkdir -p ./dags ./logs ./plugins ./config
echo -e "AIRFLOW_UID=$(id -u)" > .env

//docker-compose 실행 
docker compose up airflow-init
docker compose up
```

<br/>

## 빌드 이미지 확인 

docker compose up을 실행하면 여러 도커 이미지들이 한 번에 실행된다. 잘 실행되었는지 확인헤보자. 

```
docker ps 
```

실행되는 이미지들은 다음과 같다. 

- airflow-scheduler
- airflow-webserver
- airflow-worker
- airflow-trigger
- postgres
- redis

<br/>

localhost:8080 을 접속하면 airflow admin 로그인 화면을 볼 수 있다! <br/>
기본적으로 username과 패스워드는 **airflow** !!

<br/>
이렇게 하면 기본적은 airflow 세팅은 완료되었다. 이제 필요한 프로그램들을 dag 작성을 통해 제작하면 된다. dag 작성이나 docker-compose가 docker 이미지로 airflow 실행하는 방법 등은 이후 포스트에 올리겠다. 

