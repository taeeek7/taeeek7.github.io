---
layout : single
title : "우당탕탕 Airflow 도입 과정"
categories: data
tags: [ data_pipeline, data_engineer, docker]
toc: true
author_profile: false
sidebar:
    nav: "counts"
---

오늘은 데이터 파이프라인 툴 Airflow 도입 과정을 설명하겠다. 도입한지는 한참됐지만 이제야 정리를 해본다 ... 

### Airflow란?
Airflow는 초기 에어비엔비(Airfbnb) 엔지니어링 팀에서 개발한 워크플로우 오픈 소스 플랫폼이다.<br/>
간단히 얘기해서 스케줄을 실행해주고 여러 스케줄들이 잘 돌아가는지 쉽게 모니터링 할 수 있는 웹서버 등도 제공해주는 플랫폼이라고 볼 수 있다. 보통 데이터 워크플로우를 개선하거나 데이터엔지니어링의 ETL 작업을 자동화하기 위해 많이 사용한다. 
<br/>
<br/>

### 도입 과정
Airflow를 운영하는 방법에는 여러가지가 있지만 나는 공식 홈페이지에도 나와있는 가장 간단한 방법으로 우선 진행했다. 이는 docker-compose를 활용하여 웹서버를 띄우는 방식인데, docker가 처음이어서 생소하긴 했지만 공식 문서로도 잘 설명되어 있어서 차근차근 따라해보았다. 

## 1. Airflow 설치 
airflow를 docker-compose로 실행하기 위해 설치하는건 간단하다. 아래 내용을 따라해보자 

```
//airflow docker-compose 생성
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.9.3/docker-compose.yaml'

//디렉토리 생성 및 airflow UID 확인
mkdir -p ./dags ./logs ./plugins ./config
echo -e "AIRFLOW_UID=$(id -u)" > .env

//docker-compose 실행 
docker compose up airflow-init
docker compose up
```




